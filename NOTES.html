<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>in</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="welcome">Welcome</h1>
<!-- https://mconverter.eu/convert/markdown/html/ -->

<p>Implementing Agentic AI Solutions in Python from scratch</p>
<p>The repo is here:
https://github.com/Python-Test-Engineer/conf42-ai-agents</p>
<br>
And I will be using NOTES.md/NOTES.html for this talk along with code and the talk will be mostly code walkthroughs.

<h2 id="who-am-i">Who am I?</h2>
<p><strong>I am one of <em>US</em> - a regular Pythonista.</strong></p>
<p>I was in tech in the early 2000s as a Business Information Architect
and Certified MicroSoft SQL Server DBA. I returned in 2017 via WordPress
and JavaScript Frameworks, moving to Python and ML in 2021.</p>
<p>Currently, I am working on a project ‘AI Powered Knowledge Systems’,
building a book/framework similiar to PFS.</p>
<p>My links:</p>
<ul>
<li>https://ai-powered-knowledge-systems.netlify.app/</li>
<li>https://pytest-cookbook.com/</li>
<li>https://django-fullstack-testing.netlify.app/</li>
</ul>
<h3 id="brighton-uk">Brighton, UK</h3>
<img src="./images/brighton.jpg" width="400" height="400">

<h3 id="volounteer-coach">Volounteer coach</h3>
<p>I am a volounteer coach at codebar.io/brighton</p>
<img src="./images/codebar.png" width="400" >

<p>and I also enjoy working in community kitchens and partner
dancing.</p>
<h3 id="leo">Leo!</h3>
<p>Just got a Red Fox Labrador Pup Leo, (much earlier than planned):</p>
<p><img src="./images/leo-carrot.png" alt="Leo" /></p>
<p>We have a local red fox that is apt to follow us…</p>
<img src="./images/leo-fox-2.png" width="400" >

<h3 id="my-first-computer-1979">My first computer 1979</h3>
<p>!<img src="./images/paper-tape.jpg" width="400" ></p>
<p>https://en.wikipedia.org/wiki/Punched_tape#/media/File:Creed_model_6S-2_paper_tape_reader.jpg</p>
<p>…cut and paste was cut and paste!</p>
<h1 id="what-are-ai-agents">What are AI Agents?</h1>
<p>There are many definitions:</p>
<h2 id="pydantic">Pydantic</h2>
<p><img src="./images/what-is-agent-pydantic.png" alt="Pydantic" /></p>
<h2 id="anthropic">Anthropic</h2>
<p><img src="./images/what-is-agent-anthropic.png"
alt="Anthropic" /></p>
<h2 id="huggingface">HuggingFace</h2>
<p><img src="./images/what-is-agent-huggingface.png" alt="HF" /></p>
<p>We will look at examples of code to see what AI Agents are and what
they can do.</p>
<p>If we look at https://aiagentsdirectory.com/landscape we can see that
there are many examples of AI Agent Frameworks and they seem to increase
each week.</p>
<h2 id="aim">Aim</h2>
<p>What I would like to achieve in this talk is to demystify AI Agents
and AI Programming because it can seem like it is another different
world of dev.</p>
<p>What if AI Agents were ‘just’ Python code with a REST API call,
admittedly a very magical API?</p>
<p>Then, we would use day to day Python design patterns to handle the
responses we get back from the AI Agent and move on to the next
step.</p>
<p>This is the main focus of the talk - demystify and simplify - and not
to focus on an actual real workd application.</p>
<p>With that in mind, we don’t need to fully grasp the code this time
around. It is more about see the high level view and once can dig deeper
into the code offline.</p>
<h2 id="180-degrees">180 degrees</h2>
<p><img src="./images/mouse-up.jpg" alt="mouse up" /> <img
src="./images/mouse-down.jpg" alt="mouse down" /></p>
<p>I like to use the metaphor of the upside down computer mouse. When we
try to use it, it can take while to reverse our apporach. It is still
the same set of movements - left, right, up and down - but in the
opposite way to the way we are used to.</p>
<p>There are 3 areas concerning this.</p>
<ol type="1">
<li>Client side creation of endpoints (APIs) rather than server side
prebuilt endpoints.</li>
<li>Use of Natural/Human Language, in my case English to create the
code.</li>
<li>Autonomy - the LLM directs the flow of the app.</li>
</ol>
<p>Before we go into some code examples, we will refresh ourselves that
a REST API a request is sending a payload of data to a server and then
the server returns a response. This is a very simple example of a REST
API.</p>
<p>Again, this is to demystify and simplify any libraries we may import
for convenience functions.</p>
<p>Authentication takes place by passing some sort of token to the
server, usually in the headers:</p>
<pre><code>model = &quot;gpt-3.5-turbo&quot;
model_endpoint = &quot;https://api.openai.com/v1/chat/completions&quot;

headers = {
    &quot;Content-Type&quot;: &quot;application/json&quot;,
    &quot;Authorization&quot;: f&quot;Bearer {api_key}&quot;,
}

payload = {
   &quot;model&quot;: model,
   &quot;messages&quot;: [
       {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: system_prompt},
       {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_prompt},
   ],
   &quot;stream&quot;: False,
   &quot;temperature&quot;: temperature, 
}

# Use HTTP POST method
response = requests.post(
   url=model_endpoint, # The API
   headers=headers, # Headers fro authentication etc
   data=json.dumps(payload) # The request data we are sending
).json()
</code></pre>
<p>The request is a string of characters and does not contain any
objects or other data types.</p>
<p>In Python we stringify the data with json.dumps() and in JS it is
common to use json.stringify() to send the request.</p>
<p>Likewise, we get a string response.</p>
<p>We will look at <code>01_openai_api_with_requests.ipynb</code> to see
an example of getting a response from the LLM.</p>
<p><code>01_openai_api_with_requests.ipynb</code>.</p>
<p><em>There is only one endpoint.</em> We don’t use other endpoints for
differing tasks, there is just one end point and we create our custom
endpoint through prompt engineering.</p>
<p>In <code>02_api.ipynb</code>, we can see that we can get a joke from
a regular API endpoint, with the assumption that there is no AI
involved!</p>
<p>We can also ask OpenAI to tell us a joke…</p>
<p>What if we want a more complex API endpoint/route?</p>
<p>Let’s say we want to get a joke, get a rating as well as a verdict on
whether it is worthy of publishing or whether a HUMAN should make the
joke for publishing.</p>
<p>We can do this by using a prompt.</p>
<p>The prompt is the input to the AI agent.</p>
<p>The prompt can be considered to be the API route we are creating and
it it will be in Natural Language.</p>
<p>Let’s look at this file…</p>
<p>We can see this in <code>02_api.ipynb</code> where we pass a system
prompt and then a prompt to create this endpoint, specifying how we want
the data returned.</p>
<p>This is effectively a new route for the API, but instead of it being
coded on the server side by someone, it is coded on the client side,
sent with the payload AND the code is NATURAL LANGUAGE.</p>
<p>In the early days of ChatGPT, <em>prompt engineering</em> was often
demoed as hacks or tricks. Nowdays, it seems far more structured and
different LLMs use different schemas.</p>
<p>We can think of it as pseudo-code which we may write whilst
developing an app.</p>
<p>In fact, it is like a person starting a new job. They will get a
handbook of what the job involves, how to do it etc. and this is what we
are doing with the LLM.</p>
<p>We set the system prompt to guide the AI agent, and then the prompt
to create the endpoint.</p>
<p>We can have more information than necessary and this can do no harm
provide it is consistent and logical with the remaining prompt.
Obviously, there will be more token usage but with the price going down,
it is not an issue.</p>
<p>We have covered 2/3 of the AI reverse process - Client Side creation
of the route and the use of Natural Language.</p>
<p>What about Autonomy?</p>
<p>In our output, we asked the LLM to give not just a rating but a
verdict on whether it is worthy of publishing or not. This is the
<code>next</code> parameter that is returned. This is our own creation
and we can have any key name.</p>
<p>There are many software design patterns but essentially the next step
in the app has been selected by the LLM. It is the <code>if/else</code>
statement. or router.</p>
<p>In summary, this module has shown the 3 counter-intuitive steps of AI
Agents - Autonomy, Client Side Creation of the route and the use of
Natural Language.</p>
<h1 id="faqrouter">FAQ/ROUTER</h1>
<p>Sometimes we might think that AI Development is binary - it is fully
AI or not.</p>
<p>What if we can include ‘a bit of AI’ in our App? Remember, AI Agents
are snippets of code that make a request and get a response.</p>
<p>If we have a Search, FAQ or Help section, we can leverage the power
of the AI Agent to create a facility to process Natural Language.
Getting information from a form, (excluding text fields), give us
structured inout data.</p>
<p>Let’s look at <code>03_faq.ipynb</code> for a simple example.</p>
<p>I am using Gradio as a UI for this example and we can see that we
have some data in the FAQ list.</p>
<p>Obviously, this can be more involved and use structured inputs from
associated form fields, but for now lets assume that we have extracted
the relevant information.</p>
<p>This highlights an important point that we don’t need to use LLMs for
Agents. If we can get structured data from a form, then it is more
deterministic to do so. LLMs are very useful for converting Natural
Language inputs to structured data.</p>
<p>This is RAG or Retrieval Augmented Generation, where we ‘augment’ the
query with the relevant data and then the LLM ‘generates’ the response
based on the query, the data and the prompt. We tend to see RAG with
vector databases and semantic search but RAG is essentially augmenting
the LLM with our own data to ‘train’ it or ‘fine tune’.</p>
<p>We can see that we can create a powerful AI Agent that can answer
questions based on the data in the FAQ list.</p>
<p>We can further extend this to be a type of ROUTER or <em>if/else</em>
statement to provide a sense of autonomy to the app - it will direct the
flow of the app. We can have ‘Human in the Loop’ at any stage so that we
restrict the flow to approved paths.</p>
<p>We know the overall workflow of the app but not how it goes from
beginning to end. We no longer micro-manage the app but delegate steps
to the AI Agent, very much like we might manage a team member -
mciro-manage or use delegation.</p>
<p>In this example, we can let the AI Agent decide the next step to
take.</p>
<p>This was an example I had at a codebar coaching session where a
student wanted to get a job in AI/Python.</p>
<p>I asked if they had an AI department where they currently worked and
they said NO.</p>
<p>When I asked what they did, they would be the person people went to
for help in deciding which report to run and then they would run it and
send them the final report.</p>
<p>I said to them that they could create an AI version of themself for
when they were away. The app could offer a chatbot type interaction,
along with a set of structured form fields like date-to etc and then
select the best report and run it.</p>
<p>When they said “And replace me out of a job!” I said “yes..you will
have a new job as head of the AI dept team”.</p>
<p>Let’s look at <code>04_agent_router.ipynb</code> as a very basic
example of what they could do…I have included additional non-relevant
reports/actions for demonstration purposes.</p>
<p>We have not yet seen a multi agent scenario but I would describe this
as everyday Python where we can use a range of Software Design Patterns
like Author, Pub/Sub, Finite State Machine etc.</p>
<p>We will take a look at this later.</p>
<p>I think we can see that what we call these things - Agents, Tools,
Routers - is quite arbitrary and merely a convenience for what works for
us. At the end of the day, everything in Python is an OBJECT, so we can
use whatever we want.</p>
<h1 id="tools">Tools</h1>
<p><code>05_tools.ipynb</code> shows not just how we define tools but
also how an Agent can decide which one to use.</p>
<p>It will then send back the function name and arguments for us to run
and then return the result.</p>
<p>Where is it run?</p>
<h2 id="from-openai-website">From OpenAI website</h2>
<p>Function is run on our ‘box’ - we continue to add messages to our
list of messages and send them to the LLM.</p>
<p><img src="./images/where-tools-are-executed.png" alt="open-ai" /></p>
<h1 id="4-main-patterns">4 main patterns</h1>
<p>Andrew Ng describes four main patterns</p>
<p>https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/</p>
<p><img src="./images/4-patterns.png" alt="patterns" /></p>
<p>We have seen examples of these in this talk, bar a multi-agent
pattern.</p>
<h2 id="reflection-pattern">REFLECTION PATTERN</h2>
<p>Many times when we use ChatGPT say, we ask for it to refine its
previous answer. This is the Reflection pattern where we send the
previous response and then ask it to refine it.</p>
<p>In fact, it is like RAG. The first request generates some content,
which we then add to the next request which has a system prompt of being
a critique.</p>
<p>We can repeat this as many times as we want, adding previous
responses to the next request.</p>
<p><code>15_reflection_pattern.ipynb</code> shows how we can use this
pattern.</p>
<h2 id="tool">TOOL</h2>
<p>We have seen Tool Calling previously.</p>
<h2 id="planning">PLANNING</h2>
<p>A powerful pattern is the ReAct (Reason-Act) pattern.</p>
<p>This can be viewed of as Multi-Step. Let’s go through the code.</p>
<h2 id="multi-agent">MULTI AGENT</h2>
<h3 id="libraries">Libraries</h3>
<p>I like to think of Libraries as frameworks without the framework! By
this I mean we get building blocks to help us build things without
having to conform to a building plan.</p>
<h4 id="pydantic-ai">Pydantic AI</h4>
<p>Pydantic is well known in everyday Python and is used by most AI
Agent frameworks as structured data validation is vital.</p>
<p>PydanticAI is a library/framework that uses Pydantic to create AI
Agents.</p>
<h4 id="huggingface-smolagents">Huggingface SmolAgents</h4>
<p>HF SmolAgents is a library/framework that uses Huggingface
Transformers to create AI Agents. It has broken new ground throught the
use of its CodeAgent where tool calling is done via Python rather than
JSON…show images…</p>
<h3 id="crewsswarms">Crews/Swarms</h3>
<p>Crews and Swarms are design patterns for MultiAgent collaboration.
They each have their own use cases and we saw earlier that AI Agents can
emit the ‘next’ step in the app which a range of desing patterns can
harness.</p>
<ul>
<li>https://aiagentsdirectory.com/category/ai-agents-frameworks</li>
</ul>
<h2 id="frameworks">Frameworks</h2>
<p>There are many frameworks and libraries that can be used to create AI
Agents. Some are more focused on the AI Agent and some are more focused
on the UI.</p>
<p>LlamaIndex Langchain Langraph AutoGen CrewAI</p>
<p>And there are many low/no code versions.</p>
<h1 id="summary">Summary</h1>
<p>I hope AI Agents have been demystified and helped us understand what
they can do, enabling us to either build our own frameworks or use
existing ones, with a deeper appreciation and understanding of how they
work.</p>
<p><img src="./images/when-to-use-anthropic.png"
alt="when to use" /></p>
</body>
</html>
